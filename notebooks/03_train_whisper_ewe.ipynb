{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d18cb3d5",
            "metadata": {},
            "source": [
                "# Étape 3 : Entraînement et Test de la Cascade de Traduction\n",
                "\n",
                "Ce notebook permet de lancer l'entraînement ASR (Speech-to-Text) sur CPU et de tester la chaîne de traduction finale (Mina ➔ Éwé ➔ Français)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "5b26b6cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import importlib\n",
                "from pathlib import Path\n",
                "if str(Path.cwd().parent) not in sys.path:\n",
                "    sys.path.append(str(Path.cwd().parent))\n",
                "\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torchaudio\n",
                "from transformers import WhisperForConditionalGeneration, WhisperProcessor, AutoTokenizer, AutoModelForSeq2SeqLM, MarianTokenizer, MarianMTModel\n",
                "from src.config import settings\n",
                "import src.models.train_whisper_cpu\n",
                "\n",
                "# Reload de nos modules persos\n",
                "import src.models.translation_mina_ewe\n",
                "import src.models.translation_ewe_fr\n",
                "import src.pipeline.translate_cascade\n",
                "importlib.reload(src.models.train_whisper_cpu)\n",
                "importlib.reload(src.models.translation_mina_ewe)\n",
                "importlib.reload(src.models.translation_ewe_fr)\n",
                "importlib.reload(src.pipeline.translate_cascade)\n",
                "\n",
                "from src.models.train_whisper_cpu import train_whisper_on_cpu\n",
                "from src.pipeline.translate_cascade import TranslationCascade"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1",
            "metadata": {},
            "source": [
                "## 1. Entraînement ASR (Whisper sur CPU)\n",
                "\n",
                " (> **Note :** L'entraînement sur CPU est lent. Nous utilisons un gel des couches (freeze) à 90% pour accélérer le processus. Les hyperparamètres sont configurés dans `src/config/settings.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "a2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset chargé : 801 exemples.\n",
                        "--- Initialisation Fine-tuning Whisper (openai/whisper-base) ---\n",
                        "Chargement depuis C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\data\\processed\\bible_asr_dataset.csv...\n",
                        "Pré-traitement audio + texte (découpage segments + condition language)...\n",
                        "Dataset après préparation : 12578 exemples\n",
                        "Modèle : 72,593,920 params → gel de 90%\n",
                        "Pas de split test → création 90/10...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\src\\models\\train_whisper_cpu.py:224: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer = Seq2SeqTrainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Démarrage du fine-tuning...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a WhisperTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:232: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
                        "  check_backward_validity(args)\n",
                        "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [500/500 26:27:05, Epoch 0/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "      <th>Wer</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>5.372700</td>\n",
                            "      <td>5.369098</td>\n",
                            "      <td>1.060876</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>100</td>\n",
                            "      <td>5.379400</td>\n",
                            "      <td>5.364389</td>\n",
                            "      <td>1.067940</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>150</td>\n",
                            "      <td>5.322500</td>\n",
                            "      <td>5.360566</td>\n",
                            "      <td>1.068443</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>200</td>\n",
                            "      <td>5.398800</td>\n",
                            "      <td>5.357443</td>\n",
                            "      <td>1.072712</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>250</td>\n",
                            "      <td>5.294700</td>\n",
                            "      <td>5.354917</td>\n",
                            "      <td>1.071835</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>300</td>\n",
                            "      <td>5.344600</td>\n",
                            "      <td>5.352859</td>\n",
                            "      <td>1.070431</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>350</td>\n",
                            "      <td>5.389900</td>\n",
                            "      <td>5.351309</td>\n",
                            "      <td>1.070585</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>400</td>\n",
                            "      <td>5.348300</td>\n",
                            "      <td>5.350211</td>\n",
                            "      <td>1.066605</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>450</td>\n",
                            "      <td>5.327200</td>\n",
                            "      <td>5.349555</td>\n",
                            "      <td>1.066784</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>500</td>\n",
                            "      <td>5.385100</td>\n",
                            "      <td>5.349334</td>\n",
                            "      <td>1.066779</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
                        "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
                        "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
                        "  warnings.warn(\n",
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:232: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
                        "  check_backward_validity(args)\n",
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:232: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
                        "  check_backward_validity(args)\n",
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:232: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
                        "  check_backward_validity(args)\n",
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:232: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
                        "  check_backward_validity(args)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Entraînement terminé.\n",
                        "Modèle sauvegardé dans : C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\whisper-ewe-gegbe-final\n",
                        "Exportation du modèle ASR vers C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\whisper-ewe-mina-final...\n",
                        "Modèle ASR exporté avec succès !\n"
                    ]
                }
            ],
            "source": [
                "# Chargement du dataset préparé à l'étape 2\n",
                "dataset_path = settings.PROCESSED_DIR / \"bible_asr_dataset.csv\"\n",
                "\n",
                "if dataset_path.exists():\n",
                "    df = pd.read_csv(dataset_path)\n",
                "    print(f\"Dataset chargé : {len(df)} exemples.\")\n",
                "    \n",
                "    # Lancement de l'initialisation de l'entraînement\n",
                "    # Note: assurez-vous d'avoir assez de RAM\n",
                "    model, processor = train_whisper_on_cpu(None) \n",
                "    \n",
                "    # --- SAUVEGARDE DU MODÈLE ENTRAÎNÉ (EXPORT) ---\n",
                "    export_dir = settings.PROJECT_ROOT / \"models\" / \"whisper-ewe-mina-final\"\n",
                "    print(f\"Exportation du modèle ASR vers {export_dir}...\")\n",
                "    model.save_pretrained(export_dir)\n",
                "    processor.save_pretrained(export_dir)\n",
                "    print(\"Modèle ASR exporté avec succès !\")\n",
                "else:\n",
                "    print(\"Erreur : Le dataset ASR n'a pas encore été généré. Veuillez exécuter le notebook 02 d'abord.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a2_export",
            "metadata": {},
            "source": [
                "## 2. Exportation des autres modèles (NLLB et Opus-MT)\n",
                "\n",
                "Pour avoir une solution 100% autonome, nous téléchargeons et sauvegardons localement les modèles de traduction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "a2_export_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Téléchargement et export des modèles de traduction...\n",
                        "Modèle NLLB déjà présent dans C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\nllb-mina-ewe-local\n",
                        "Modèle Opus déjà présent dans C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\opus-ewe-fr-local\n",
                        "Tous les modèles sont prêts localement !\n"
                    ]
                }
            ],
            "source": [
                "print(\"Téléchargement et export des modèles de traduction...\")\n",
                "\n",
                "# 1. Export NLLB (Mina -> Ewe)\n",
                "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
                "nllb_path = settings.PROJECT_ROOT / \"models\" / \"nllb-mina-ewe-local\"\n",
                "\n",
                "if not nllb_path.exists():\n",
                "    print(f\"Téléchargement de {nllb_model_name}...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name)\n",
                "    \n",
                "    print(f\"Sauvegarde dans {nllb_path}...\")\n",
                "    model.save_pretrained(nllb_path)\n",
                "    tokenizer.save_pretrained(nllb_path)\n",
                "else:\n",
                "    print(f\"Modèle NLLB déjà présent dans {nllb_path}\")\n",
                "\n",
                "# 2. Export Opus-MT (Ewe -> Français)\n",
                "opus_model_name = \"Helsinki-NLP/opus-mt-ee-fr\"\n",
                "opus_path = settings.PROJECT_ROOT / \"models\" / \"opus-ewe-fr-local\"\n",
                "\n",
                "if not opus_path.exists():\n",
                "    print(f\"Téléchargement de {opus_model_name}...\")\n",
                "    tokenizer = MarianTokenizer.from_pretrained(opus_model_name)\n",
                "    model = MarianMTModel.from_pretrained(opus_model_name)\n",
                "    \n",
                "    print(f\"Sauvegarde dans {opus_path}...\")\n",
                "    model.save_pretrained(opus_path)\n",
                "    tokenizer.save_pretrained(opus_path)\n",
                "else:\n",
                "    print(f\"Modèle Opus déjà présent dans {opus_path}\")\n",
                "\n",
                "print(\"Tous les modèles sont prêts localement !\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5",
            "metadata": {},
            "source": [
                "## 3. Test Audio Complet avec les 3 Modèles Exportés (Local)\n",
                "\n",
                "Nous chargeons maintenant **les 3 modèles** uniquement depuis le dossier `models/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "a6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1. Chargement du modèle ASR local...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Initialisation de la cascade de traduction...\n",
                        "INFO:src.models.translation_mina_ewe:Chargement du modèle Mina-Ewe : C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\nllb-mina-ewe-local\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   OK.\n",
                        "2. Initialisation de la Cascade avec les modèles locaux...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer you are loading from 'C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\nllb-mina-ewe-local' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
                        "INFO:src.models.translation_ewe_fr:Chargement du modèle Transformers C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\opus-ewe-fr-local\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   OK.\n"
                    ]
                }
            ],
            "source": [
                "# Chemins locaux\n",
                "asr_path = settings.PROJECT_ROOT / \"models\" / \"whisper-ewe-mina-final\"\n",
                "nllb_path = settings.PROJECT_ROOT / \"models\" / \"nllb-mina-ewe-local\"\n",
                "opus_path = settings.PROJECT_ROOT / \"models\" / \"opus-ewe-fr-local\"\n",
                "\n",
                "print(\"1. Chargement du modèle ASR local...\")\n",
                "try:\n",
                "    loaded_processor = WhisperProcessor.from_pretrained(asr_path)\n",
                "    loaded_model = WhisperForConditionalGeneration.from_pretrained(asr_path)\n",
                "    print(\"   OK.\")\n",
                "except Exception as e:\n",
                "    print(f\"   Erreur ASR (il faut lancer l'entraînement avant) : {e}\")\n",
                "    # Fallback pour ne pas bloquer si pas entraîné dans cette session\n",
                "    loaded_processor = processor \n",
                "    loaded_model = model\n",
                "\n",
                "print(\"2. Initialisation de la Cascade avec les modèles locaux...\")\n",
                "cascade = TranslationCascade(nllb_path=str(nllb_path), opus_path=str(opus_path))\n",
                "print(\"   OK.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfed416a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Traitement audio : gegbe_gen_01.wav\n",
                        "Durée originale : 368.40 secondes\n",
                        "Échantillons    : 5,894,400\n",
                        "Sample rate lu  : 16000 Hz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Source (Mina): Tɔhonɔ gblɔ na Mosè ku Arɔn be Izràɛlviwo\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Transcription (ASR) :\n",
                        " Gomejdejbehoma Eta tutu guan Hihiabe dodo So gomejdejbehma Mauro d'yunku siku aniban Aniwa leñamaa Eibeleqbalo Viviti tri tri porla gomejdejbea pubajah Va maubhe bongboa di nasa leciojis Maughe bongbi\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Pivot (Ewe): Tɔhonɔ gblɔ na Mose kple Aron be woanye Israelviwo\n",
                        "INFO:src.pipeline.translate_cascade:Cible (Français): Moïse et Aaron ont reçu l'ordre d'être des Israélites.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== RÉSULTAT 100% LOCAL ===\n",
                        "TEXTE (ASR) : Tɔhonɔ gblɔ na Mosè ku Arɔn be Izràɛlviwo\n",
                        "PIVOT (EWE) : Tɔhonɔ gblɔ na Mose kple Aron be woanye Israelviwo\n",
                        "TRAD (FR)   : Moïse et Aaron ont reçu l'ordre d'être des Israélites.\n"
                    ]
                }
            ],
            "source": [
                "# --- INFERENCE ---\n",
                "import soundfile as sf\n",
                "import scipy.signal\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "sample_audio = settings.PROCESSED_DIR / \"audio_16k\" / \"gegbe_gen_01.wav\"\n",
                "\n",
                "if sample_audio.exists():\n",
                "    print(f\"\\nTraitement audio : {sample_audio.name}\")\n",
                "    \n",
                "    waveform, sr = sf.read(sample_audio)\n",
                "    \n",
                "    print(f\"Durée originale : {len(waveform) / sr:.2f} secondes\")\n",
                "    print(f\"Échantillons    : {len(waveform):,}\")\n",
                "    print(f\"Sample rate lu  : {sr} Hz\")\n",
                "    \n",
                "    # Mono si stéréo\n",
                "    if len(waveform.shape) > 1:\n",
                "        waveform = waveform.mean(axis=1)\n",
                "        \n",
                "    # Resample si besoin\n",
                "    if sr != 16000:\n",
                "        num_samples = int(round(len(waveform) * 16000 / sr))\n",
                "        waveform = scipy.signal.resample(waveform, num_samples)\n",
                "    \n",
                "    # Préparation Whisper\n",
                "    input_features = loaded_processor(\n",
                "        waveform,\n",
                "        sampling_rate=16000,\n",
                "        return_tensors=\"pt\"\n",
                "    ).input_features\n",
                "    \n",
                "    # Très important : déplacer sur le bon device (GPU si disponible)\n",
                "    device = next(loaded_model.parameters()).device\n",
                "    input_features = input_features.to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        predicted_ids = loaded_model.generate(\n",
                "            input_features,\n",
                "            language=None,                     # ou \"mina\" / None si détection auto\n",
                "            task=\"transcribe\",\n",
                "            return_timestamps=True,             # active le découpage automatique 30s + stitching\n",
                "            condition_on_prev_tokens=True,\n",
                "            temperature=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],  # fallback si répétitions/hallucinations\n",
                "            compression_ratio_threshold=1.35,\n",
                "            logprob_threshold=-1.0,\n",
                "            no_speech_threshold=0.6,\n",
                "        )\n",
                "    \n",
                "    transcription = loaded_processor.batch_decode(\n",
                "        predicted_ids,\n",
                "        skip_special_tokens=True\n",
                "    )[0]\n",
                "    \n",
                "    print(f\"\\nTranscription (ASR) :\\n{transcription}\\n\")\n",
                "    \n",
                "    final_result = cascade.translate_mina_to_french(transcription)\n",
                "    \n",
                "    print(\"=== RÉSULTAT 100% LOCAL ===\")\n",
                "    print(f\"TEXTE (ASR) : {final_result['mina']}\")\n",
                "    print(f\"PIVOT (EWE) : {final_result['ewe']}\")\n",
                "    print(f\"TRAD (FR)   : {final_result['french']}\")\n",
                "\n",
                "else:\n",
                "    print(\"Fichier audio non trouvé.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f1d33a2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- ATTENTION : Enregistrement de 5 secondes ---\n",
                        "Parlez maintenant...\n",
                        "Enregistrement terminé. Traitement en cours...\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Source (Mina):  Apitonila\n",
                        "INFO:src.pipeline.translate_cascade:Pivot (Ewe): Apitonila\n",
                        "INFO:src.pipeline.translate_cascade:Cible (Français): Aptonaila\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== RÉSULTAT 100% LOCAL (MICRO) ===\n",
                        "TEXTE (ASR) :  Apitonila\n",
                        "PIVOT (EWE) : Apitonila\n",
                        "TRAD (FR)   : Aptonaila\n"
                    ]
                }
            ],
            "source": [
                "import sounddevice as sd\n",
                "import numpy as np\n",
                "\n",
                "# --- CONFIGURATION MICRO ---\n",
                "fs = 16000  # Fréquence d'échantillonnage requise par Whisper\n",
                "duration = 5  # Durée de l'enregistrement en secondes (à ajuster)\n",
                "\n",
                "print(f\"\\n--- ATTENTION : Enregistrement de {duration} secondes ---\")\n",
                "print(\"Parlez maintenant...\")\n",
                "\n",
                "# Capture de l'audio\n",
                "# sd.rec enregistre dans un array NumPy\n",
                "recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
                "sd.wait()  # Attend que l'enregistrement soit terminé\n",
                "\n",
                "print(\"Enregistrement terminé. Traitement en cours...\\n\")\n",
                "\n",
                "# Conversion en mono (déjà fait par channels=1, mais on aplatit l'array)\n",
                "waveform = recording.flatten()\n",
                "\n",
                "# --- REPRISE DE TON PROCESSUS INFERENCE ---\n",
                "\n",
                "# Préparation Whisper (plus besoin de resample si fs=16000)\n",
                "input_features = loaded_processor(\n",
                "    waveform,\n",
                "    sampling_rate=fs,\n",
                "    return_tensors=\"pt\"\n",
                ").input_features\n",
                "\n",
                "# Déplacement sur le bon device\n",
                "device = next(loaded_model.parameters()).device\n",
                "input_features = input_features.to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    predicted_ids = loaded_model.generate(\n",
                "        input_features,\n",
                "        language=None, # Auto-détection\n",
                "        task=\"transcribe\",\n",
                "        return_timestamps=True,\n",
                "        condition_on_prev_tokens=True,\n",
                "        temperature=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
                "        compression_ratio_threshold=1.35,\n",
                "        logprob_threshold=-1.0,\n",
                "        no_speech_threshold=0.6,\n",
                "    )\n",
                "\n",
                "transcription = loaded_processor.batch_decode(\n",
                "    predicted_ids,\n",
                "    skip_special_tokens=True\n",
                ")[0]\n",
                "\n",
                "# --- TRADUCTION ---\n",
                "if transcription.strip():\n",
                "    final_result = cascade.translate_mina_to_french(transcription)\n",
                "    \n",
                "    print(\"=== RÉSULTAT 100% LOCAL (MICRO) ===\")\n",
                "    print(f\"TEXTE (ASR) : {final_result['mina']}\")\n",
                "    print(f\"PIVOT (EWE) : {final_result['ewe']}\")\n",
                "    print(f\"TRAD (FR)   : {final_result['french']}\")\n",
                "else:\n",
                "    print(\"Aucun son détecté ou transcription vide.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ec0367f",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
