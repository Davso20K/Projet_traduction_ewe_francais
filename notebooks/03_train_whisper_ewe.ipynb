{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d18cb3d5",
            "metadata": {},
            "source": [
                "# Étape 3 : Entraînement et Test de la Cascade de Traduction\n",
                "\n",
                "Ce notebook permet de lancer l'entraînement ASR (Speech-to-Text) sur CPU et de tester la chaîne de traduction finale (Mina ➔ Éwé ➔ Français)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5b26b6cf",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import importlib\n",
                "from pathlib import Path\n",
                "if str(Path.cwd().parent) not in sys.path:\n",
                "    sys.path.append(str(Path.cwd().parent))\n",
                "\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torchaudio\n",
                "from transformers import WhisperForConditionalGeneration, WhisperProcessor, AutoTokenizer, AutoModelForSeq2SeqLM, MarianTokenizer, MarianMTModel\n",
                "from src.config import settings\n",
                "import src.models.train_whisper_cpu\n",
                "\n",
                "# Reload de nos modules persos\n",
                "import src.models.translation_mina_ewe\n",
                "import src.models.translation_ewe_fr\n",
                "import src.pipeline.translate_cascade\n",
                "importlib.reload(src.models.train_whisper_cpu)\n",
                "importlib.reload(src.models.translation_mina_ewe)\n",
                "importlib.reload(src.models.translation_ewe_fr)\n",
                "importlib.reload(src.pipeline.translate_cascade)\n",
                "\n",
                "from src.models.train_whisper_cpu import train_whisper_on_cpu\n",
                "from src.pipeline.translate_cascade import TranslationCascade"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1",
            "metadata": {},
            "source": [
                "## 1. Entraînement ASR (Whisper sur CPU)\n",
                "\n",
                " (> **Note :** L'entraînement sur CPU est lent. Nous utilisons un gel des couches (freeze) à 90% pour accélérer le processus. Les hyperparamètres sont configurés dans `src/config/settings.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "a2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset chargé : 23559 exemples.\n",
                        "--- Initialisation Fine-tuning Whisper (openai/whisper-base) sur CPU (12 coeurs) ---\n",
                        "Chargement depuis C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\data\\processed\\bible_asr_dataset.csv...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Generating train split: 23559 examples [00:00, 174983.94 examples/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Pré-traitement du dataset (Feature Extraction)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map (num_proc=10):   0%|          | 0/23559 [00:29<?, ? examples/s]\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback].",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mRemoteTraceback\u001b[39m                           Traceback (most recent call last)",
                        "\u001b[31mRemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 586, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3675, in _map_single\n    for i, example in iter_outputs(shard_iterable):\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3649, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3572, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\src\\models\\train_whisper_cpu.py\", line 80, in prepare_dataset\n    audio = batch[\"audio_filepath\"]\n            ~~~~~^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 285, in __getitem__\n    value = self.format(key)\n            ^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 380, in format\n    return self.formatter.format_column(self.pa_table.select([key]))[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 465, in format_column\n    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 228, in decode_column\n    self.features.decode_column(column, column_name, token_per_repo_id=self.token_per_repo_id)\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\features\\features.py\", line 2134, in decode_column\n    [\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\features\\features.py\", line 2135, in <listcomp>\n    decode_nested_example(self[column_name], value, token_per_repo_id=token_per_repo_id)\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\features\\features.py\", line 1419, in decode_nested_example\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\features\\audio.py\", line 184, in decode_example\n    from ._torchcodec import AudioDecoder\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\features\\_torchcodec.py\", line 2, in <module>\n    from torchcodec.decoders import AudioDecoder as _AudioDecoder\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\__init__.py\", line 12, in <module>\n    from . import decoders, encoders, samplers  # noqa\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\decoders\\__init__.py\", line 7, in <module>\n    from .._core import AudioStreamMetadata, VideoStreamMetadata\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\_core\\__init__.py\", line 8, in <module>\n    from ._metadata import (\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\_core\\_metadata.py\", line 16, in <module>\n    from torchcodec._core.ops import (\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 104, in <module>\n    ffmpeg_major_version, core_library_path = load_torchcodec_shared_libraries()\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\_core\\ops.py\", line 75, in load_torchcodec_shared_libraries\n    raise RuntimeError(\nRuntimeError: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback].\n\"\"\"",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset chargé : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exemples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Lancement de l'initialisation de l'entraînement\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Note: assurez-vous d'avoir assez de RAM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model, processor = \u001b[43mtrain_whisper_on_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# --- SAUVEGARDE DU MODÈLE ENTRAÎNÉ (EXPORT) ---\u001b[39;00m\n\u001b[32m     13\u001b[39m export_dir = settings.PROJECT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mwhisper-ewe-mina-final\u001b[39m\u001b[33m\"\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\src\\models\\train_whisper_cpu.py:115\u001b[39m, in \u001b[36mtrain_whisper_on_cpu\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPré-traitement du dataset (Feature Extraction)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# On utilise num_proc pour paralléliser le prétraitement\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_CORES\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset prêt : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exemples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# 3. Split\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\dataset_dict.py:953\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    951\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    975\u001b[39m     function = function.func\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3334\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3331\u001b[39m os.environ = prev_env\n\u001b[32m   3332\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m processes\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3334\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43munprocessed_kwargs_per_job\u001b[49m\n\u001b[32m   3336\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3339\u001b[39m pool.close()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py:626\u001b[39m, in \u001b[36miflatmap_unordered\u001b[39m\u001b[34m(pool, func, kwargs_iterable)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         \u001b[43m[\u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_results\u001b[49m\u001b[43m]\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py:626\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[32m    625\u001b[39m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         [\u001b[43masync_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\multiprocess\\pool.py:774\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
                        "\u001b[31mRuntimeError\u001b[39m: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6, 7, and 8. On Windows, ensure you've installed\n             the \"full-shared\" version which ships DLLs.\n          2. The PyTorch version (2.10.0+cpu) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n        \n[start of libtorchcodec loading traceback]\nFFmpeg version 8: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core8.dll\nFFmpeg version 7: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core7.dll\nFFmpeg version 6: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core6.dll\nFFmpeg version 5: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core5.dll\nFFmpeg version 4: Could not load this library: C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\venv\\Lib\\site-packages\\torchcodec\\libtorchcodec_core4.dll\n[end of libtorchcodec loading traceback]."
                    ]
                }
            ],
            "source": [
                "# Chargement du dataset préparé à l'étape 2\n",
                "dataset_path = settings.PROCESSED_DIR / \"bible_asr_dataset.csv\"\n",
                "\n",
                "if dataset_path.exists():\n",
                "    df = pd.read_csv(dataset_path)\n",
                "    print(f\"Dataset chargé : {len(df)} exemples.\")\n",
                "    \n",
                "    # Lancement de l'initialisation de l'entraînement\n",
                "    # Note: assurez-vous d'avoir assez de RAM\n",
                "    model, processor = train_whisper_on_cpu(None) \n",
                "    \n",
                "    # --- SAUVEGARDE DU MODÈLE ENTRAÎNÉ (EXPORT) ---\n",
                "    export_dir = settings.PROJECT_ROOT / \"models\" / \"whisper-ewe-mina-final\"\n",
                "    print(f\"Exportation du modèle ASR vers {export_dir}...\")\n",
                "    model.save_pretrained(export_dir)\n",
                "    processor.save_pretrained(export_dir)\n",
                "    print(\"Modèle ASR exporté avec succès !\")\n",
                "else:\n",
                "    print(\"Erreur : Le dataset ASR n'a pas encore été généré. Veuillez exécuter le notebook 02 d'abord.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a2_export",
            "metadata": {},
            "source": [
                "## 2. Exportation des autres modèles (NLLB et Opus-MT)\n",
                "\n",
                "Pour avoir une solution 100% autonome, nous téléchargeons et sauvegardons localement les modèles de traduction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2_export_code",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Téléchargement et export des modèles de traduction...\n",
                        "Modèle NLLB déjà présent dans C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\nllb-mina-ewe-local\n",
                        "Modèle Opus déjà présent dans C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\opus-ewe-fr-local\n",
                        "Tous les modèles sont prêts localement !\n"
                    ]
                }
            ],
            "source": [
                "print(\"Téléchargement et export des modèles de traduction...\")\n",
                "\n",
                "# 1. Export NLLB (Mina -> Ewe)\n",
                "nllb_model_name = \"facebook/nllb-200-distilled-600M\"\n",
                "nllb_path = settings.PROJECT_ROOT / \"models\" / \"nllb-mina-ewe-local\"\n",
                "\n",
                "if not nllb_path.exists():\n",
                "    print(f\"Téléchargement de {nllb_model_name}...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)\n",
                "    model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name)\n",
                "    \n",
                "    print(f\"Sauvegarde dans {nllb_path}...\")\n",
                "    model.save_pretrained(nllb_path)\n",
                "    tokenizer.save_pretrained(nllb_path)\n",
                "else:\n",
                "    print(f\"Modèle NLLB déjà présent dans {nllb_path}\")\n",
                "\n",
                "# 2. Export Opus-MT (Ewe -> Français)\n",
                "opus_model_name = \"Helsinki-NLP/opus-mt-ee-fr\"\n",
                "opus_path = settings.PROJECT_ROOT / \"models\" / \"opus-ewe-fr-local\"\n",
                "\n",
                "if not opus_path.exists():\n",
                "    print(f\"Téléchargement de {opus_model_name}...\")\n",
                "    tokenizer = MarianTokenizer.from_pretrained(opus_model_name)\n",
                "    model = MarianMTModel.from_pretrained(opus_model_name)\n",
                "    \n",
                "    print(f\"Sauvegarde dans {opus_path}...\")\n",
                "    model.save_pretrained(opus_path)\n",
                "    tokenizer.save_pretrained(opus_path)\n",
                "else:\n",
                "    print(f\"Modèle Opus déjà présent dans {opus_path}\")\n",
                "\n",
                "print(\"Tous les modèles sont prêts localement !\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5",
            "metadata": {},
            "source": [
                "## 3. Test Audio Complet avec les 3 Modèles Exportés (Local)\n",
                "\n",
                "Nous chargeons maintenant **les 3 modèles** uniquement depuis le dossier `models/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1. Chargement du modèle ASR local...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Initialisation de la cascade de traduction...\n",
                        "INFO:src.models.translation_mina_ewe:Chargement du modèle Mina-Ewe : C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\nllb-mina-ewe-local\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   OK.\n",
                        "2. Initialisation de la Cascade avec les modèles locaux...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The tokenizer you are loading from 'C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\nllb-mina-ewe-local' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
                        "INFO:src.models.translation_ewe_fr:Chargement du modèle Transformers C:\\EPL\\MTH2321\\MTH_2321_APEKE\\Projet_traduction_ewe_francais\\models\\opus-ewe-fr-local\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   OK.\n"
                    ]
                }
            ],
            "source": [
                "# Chemins locaux\n",
                "asr_path = settings.PROJECT_ROOT / \"models\" / \"whisper-ewe-mina-final\"\n",
                "nllb_path = settings.PROJECT_ROOT / \"models\" / \"nllb-mina-ewe-local\"\n",
                "opus_path = settings.PROJECT_ROOT / \"models\" / \"opus-ewe-fr-local\"\n",
                "\n",
                "print(\"1. Chargement du modèle ASR local...\")\n",
                "try:\n",
                "    loaded_processor = WhisperProcessor.from_pretrained(asr_path)\n",
                "    loaded_model = WhisperForConditionalGeneration.from_pretrained(asr_path)\n",
                "    print(\"   OK.\")\n",
                "except Exception as e:\n",
                "    print(f\"   Erreur ASR (il faut lancer l'entraînement avant) : {e}\")\n",
                "    # Fallback pour ne pas bloquer si pas entraîné dans cette session\n",
                "    loaded_processor = processor \n",
                "    loaded_model = model\n",
                "\n",
                "print(\"2. Initialisation de la Cascade avec les modèles locaux...\")\n",
                "cascade = TranslationCascade(nllb_path=str(nllb_path), opus_path=str(opus_path))\n",
                "print(\"   OK.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cfed416a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Traitement audio : gegbe_gen_01.wav\n",
                        "Durée originale : 368.40 secondes\n",
                        "Échantillons    : 5,894,400\n",
                        "Sample rate lu  : 16000 Hz\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Source (Mina): Tɔhonɔ gblɔ na Mosè ku Arɔn be Izràɛlviwo\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Transcription (ASR) :\n",
                        " Gomejdejbehoma Eta tutu guan Hihiabe dodo So gomejdejbehma Mauro d'yunku siku aniban Aniwa leñamaa Eibeleqbalo Viviti tri tri porla gomejdejbea pubajah Va maubhe bongboa di nasa leciojis Maughe bongbi\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Pivot (Ewe): Tɔhonɔ gblɔ na Mose kple Aron be woanye Israelviwo\n",
                        "INFO:src.pipeline.translate_cascade:Cible (Français): Moïse et Aaron ont reçu l'ordre d'être des Israélites.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== RÉSULTAT 100% LOCAL ===\n",
                        "TEXTE (ASR) : Tɔhonɔ gblɔ na Mosè ku Arɔn be Izràɛlviwo\n",
                        "PIVOT (EWE) : Tɔhonɔ gblɔ na Mose kple Aron be woanye Israelviwo\n",
                        "TRAD (FR)   : Moïse et Aaron ont reçu l'ordre d'être des Israélites.\n"
                    ]
                }
            ],
            "source": [
                "# --- INFERENCE ---\n",
                "import soundfile as sf\n",
                "import scipy.signal\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "sample_audio = settings.PROCESSED_DIR / \"audio_16k\" / \"gegbe_gen_01.wav\"\n",
                "\n",
                "if sample_audio.exists():\n",
                "    print(f\"\\nTraitement audio : {sample_audio.name}\")\n",
                "    \n",
                "    waveform, sr = sf.read(sample_audio)\n",
                "    \n",
                "    print(f\"Durée originale : {len(waveform) / sr:.2f} secondes\")\n",
                "    print(f\"Échantillons    : {len(waveform):,}\")\n",
                "    print(f\"Sample rate lu  : {sr} Hz\")\n",
                "    \n",
                "    # Mono si stéréo\n",
                "    if len(waveform.shape) > 1:\n",
                "        waveform = waveform.mean(axis=1)\n",
                "        \n",
                "    # Resample si besoin\n",
                "    if sr != 16000:\n",
                "        num_samples = int(round(len(waveform) * 16000 / sr))\n",
                "        waveform = scipy.signal.resample(waveform, num_samples)\n",
                "    \n",
                "    # Préparation Whisper\n",
                "    input_features = loaded_processor(\n",
                "        waveform,\n",
                "        sampling_rate=16000,\n",
                "        return_tensors=\"pt\"\n",
                "    ).input_features\n",
                "    \n",
                "    # Très important : déplacer sur le bon device (GPU si disponible)\n",
                "    device = next(loaded_model.parameters()).device\n",
                "    input_features = input_features.to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        predicted_ids = loaded_model.generate(\n",
                "            input_features,\n",
                "            language=None,                     # ou \"mina\" / None si détection auto\n",
                "            task=\"transcribe\",\n",
                "            return_timestamps=True,             # active le découpage automatique 30s + stitching\n",
                "            condition_on_prev_tokens=True,\n",
                "            temperature=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],  # fallback si répétitions/hallucinations\n",
                "            compression_ratio_threshold=1.35,\n",
                "            logprob_threshold=-1.0,\n",
                "            no_speech_threshold=0.6,\n",
                "        )\n",
                "    \n",
                "    transcription = loaded_processor.batch_decode(\n",
                "        predicted_ids,\n",
                "        skip_special_tokens=True\n",
                "    )[0]\n",
                "    \n",
                "    print(f\"\\nTranscription (ASR) :\\n{transcription}\\n\")\n",
                "    \n",
                "    final_result = cascade.translate_mina_to_french(transcription)\n",
                "    \n",
                "    print(\"=== RÉSULTAT 100% LOCAL ===\")\n",
                "    print(f\"TEXTE (ASR) : {final_result['mina']}\")\n",
                "    print(f\"PIVOT (EWE) : {final_result['ewe']}\")\n",
                "    print(f\"TRAD (FR)   : {final_result['french']}\")\n",
                "\n",
                "else:\n",
                "    print(\"Fichier audio non trouvé.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f1d33a2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- ATTENTION : Enregistrement de 5 secondes ---\n",
                        "Parlez maintenant...\n",
                        "Enregistrement terminé. Traitement en cours...\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.pipeline.translate_cascade:Source (Mina):  Apitonila\n",
                        "INFO:src.pipeline.translate_cascade:Pivot (Ewe): Apitonila\n",
                        "INFO:src.pipeline.translate_cascade:Cible (Français): Aptonaila\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== RÉSULTAT 100% LOCAL (MICRO) ===\n",
                        "TEXTE (ASR) :  Apitonila\n",
                        "PIVOT (EWE) : Apitonila\n",
                        "TRAD (FR)   : Aptonaila\n"
                    ]
                }
            ],
            "source": [
                "import sounddevice as sd\n",
                "import numpy as np\n",
                "\n",
                "# --- CONFIGURATION MICRO ---\n",
                "fs = 16000  # Fréquence d'échantillonnage requise par Whisper\n",
                "duration = 5  # Durée de l'enregistrement en secondes (à ajuster)\n",
                "\n",
                "print(f\"\\n--- ATTENTION : Enregistrement de {duration} secondes ---\")\n",
                "print(\"Parlez maintenant...\")\n",
                "\n",
                "# Capture de l'audio\n",
                "# sd.rec enregistre dans un array NumPy\n",
                "recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float32')\n",
                "sd.wait()  # Attend que l'enregistrement soit terminé\n",
                "\n",
                "print(\"Enregistrement terminé. Traitement en cours...\\n\")\n",
                "\n",
                "# Conversion en mono (déjà fait par channels=1, mais on aplatit l'array)\n",
                "waveform = recording.flatten()\n",
                "\n",
                "# --- REPRISE DE TON PROCESSUS INFERENCE ---\n",
                "\n",
                "# Préparation Whisper (plus besoin de resample si fs=16000)\n",
                "input_features = loaded_processor(\n",
                "    waveform,\n",
                "    sampling_rate=fs,\n",
                "    return_tensors=\"pt\"\n",
                ").input_features\n",
                "\n",
                "# Déplacement sur le bon device\n",
                "device = next(loaded_model.parameters()).device\n",
                "input_features = input_features.to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    predicted_ids = loaded_model.generate(\n",
                "        input_features,\n",
                "        language=None, # Auto-détection\n",
                "        task=\"transcribe\",\n",
                "        return_timestamps=True,\n",
                "        condition_on_prev_tokens=True,\n",
                "        temperature=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
                "        compression_ratio_threshold=1.35,\n",
                "        logprob_threshold=-1.0,\n",
                "        no_speech_threshold=0.6,\n",
                "    )\n",
                "\n",
                "transcription = loaded_processor.batch_decode(\n",
                "    predicted_ids,\n",
                "    skip_special_tokens=True\n",
                ")[0]\n",
                "\n",
                "# --- TRADUCTION ---\n",
                "if transcription.strip():\n",
                "    final_result = cascade.translate_mina_to_french(transcription)\n",
                "    \n",
                "    print(\"=== RÉSULTAT 100% LOCAL (MICRO) ===\")\n",
                "    print(f\"TEXTE (ASR) : {final_result['mina']}\")\n",
                "    print(f\"PIVOT (EWE) : {final_result['ewe']}\")\n",
                "    print(f\"TRAD (FR)   : {final_result['french']}\")\n",
                "else:\n",
                "    print(\"Aucun son détecté ou transcription vide.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ec0367f",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
